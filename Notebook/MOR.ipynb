{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "\n%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\n%%configure\n{\n    \"--conf\": \"spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\",\n    \"--enable-glue-datacatalog\" :\"true\",\n    \"--datalake-formats\":\"hudi\"\n}",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session c78fb162-241e-4e1b-b6e0-3a332def3078.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session c78fb162-241e-4e1b-b6e0-3a332def3078.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 4.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session c78fb162-241e-4e1b-b6e0-3a332def3078.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session c78fb162-241e-4e1b-b6e0-3a332def3078.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session c78fb162-241e-4e1b-b6e0-3a332def3078.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--conf': 'spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false', '--enable-glue-datacatalog': 'true', '--datalake-formats': 'hudi'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"show databases;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+\n|namespace|\n+---------+\n|  default|\n|  hudidb1|\n|  hudidb2|\n|  hudidb3|\n+---------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "try:\n    import os\n    import sys\n\n\n    import pyspark\n    from pyspark import SparkConf, SparkContext\n    from pyspark.sql import SparkSession\n    from pyspark.sql.functions import col, asc, desc\n    from awsglue.utils import getResolvedOptions\n    from awsglue.dynamicframe import DynamicFrame\n    from awsglue.context import GlueContext\n\n    from faker import Faker\n\n    print(\"All modules are loaded .....\")\n\nexcept Exception as e:\n    print(\"Some modules are missing {} \".format(e))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Some modules are missing No module named 'faker'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "database_name1 = \"hudidb7\"\ntable_name = \"hudi_table\"\nbase_s3_path = \"s3a://test-ramneek-2\"\nfinal_base_path = \"{base_s3_path}/{table_name}\".format(\n    base_s3_path=base_s3_path, table_name=table_name\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "class DataGenerator(object):\n\n    @staticmethod\n    def get_data():\n        # Manually created data\n        return [\n            (1, \"Alice Johnson\", \"IT\", \"CA\", 120000, 30, 5000, 1677624870),\n            (2, \"Bob Smith\", \"HR\", \"NY\", 90000, 40, 7000, 1677624871),\n            (3, \"Charlie Lee\", \"Sales\", \"TX\", 110000, 35, 8000, 1677624872),\n            (4, \"David Brown\", \"Marketing\", \"FL\", 95000, 29, 4000, 1677624873),\n            (5, \"Eve Davis\", \"IT\", \"IL\", 105000, 32, 6000, 1677624874)\n        ]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def create_spark_session():\n    spark = SparkSession \\\n        .builder \\\n        .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n        .getOrCreate()\n    return spark\n\n\nspark = create_spark_session()\nsc = spark.sparkContext\nglueContext = GlueContext(sc)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "hudi_options = {\n    'hoodie.table.name': table_name,\n    \"hoodie.datasource.write.storage.type\": \"MERGE_ON_READ\",\n    'hoodie.datasource.write.recordkey.field': 'emp_id',\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': database_name1,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data = DataGenerator.get_data()\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\ndf = spark.createDataFrame(data=data, schema=columns)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+-------------+----------+-----+------+---+-----+----------+\n|emp_id|employee_name|department|state|salary|age|bonus|        ts|\n+------+-------------+----------+-----+------+---+-----+----------+\n|     1|Alice Johnson|        IT|   CA|120000| 30| 5000|1677624870|\n|     2|    Bob Smith|        HR|   NY| 90000| 40| 7000|1677624871|\n|     3|  Charlie Lee|     Sales|   TX|110000| 35| 8000|1677624872|\n|     4|  David Brown| Marketing|   FL| 95000| 29| 4000|1677624873|\n|     5|    Eve Davis|        IT|   IL|105000| 32| 6000|1677624874|\n+------+-------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"show databases;\").show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+\n|namespace|\n+---------+\n|  default|\n|  hudidb1|\n|  hudidb2|\n|  hudidb3|\n|  hudidb7|\n+---------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"use hudidb7;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"show tables;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+-------------+-----------+\n|namespace|    tableName|isTemporary|\n+---------+-------------+-----------+\n|  hudidb7|hudi_table_ro|      false|\n|  hudidb7|hudi_table_rt|      false|\n+---------+-------------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from hudi_table_rt;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|employee_name|department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n|  20240927075321357|20240927075321357...|                 1|                      |25309ccf-dec5-4ae...|     1|Alice Johnson|        IT|   CA|120000| 30| 5000|1677624870|\n|  20240927075321357|20240927075321357...|                 5|                      |25309ccf-dec5-4ae...|     5|    Eve Davis|        IT|   IL|105000| 32| 6000|1677624874|\n|  20240927075321357|20240927075321357...|                 3|                      |25309ccf-dec5-4ae...|     3|  Charlie Lee|     Sales|   TX|110000| 35| 8000|1677624872|\n|  20240927075321357|20240927075321357...|                 2|                      |25309ccf-dec5-4ae...|     2|    Bob Smith|        HR|   NY| 90000| 40| 7000|1677624871|\n|  20240927075321357|20240927075321357...|                 4|                      |25309ccf-dec5-4ae...|     4|  David Brown| Marketing|   FL| 95000| 29| 4000|1677624873|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3_parquet_path = \"s3://test-ramneek-2/hudi_table/25309ccf-dec5-4ae1-8f39-f6391e564bcb-0_0-84-348_20240927075321357.parquet\"\n\n# Read the Parquet file\ndf1 = spark.read.parquet(s3_parquet_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df1.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|employee_name|department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n|  20240927075321357|20240927075321357...|                 1|                      |25309ccf-dec5-4ae...|     1|Alice Johnson|        IT|   CA|120000| 30| 5000|1677624870|\n|  20240927075321357|20240927075321357...|                 5|                      |25309ccf-dec5-4ae...|     5|    Eve Davis|        IT|   IL|105000| 32| 6000|1677624874|\n|  20240927075321357|20240927075321357...|                 3|                      |25309ccf-dec5-4ae...|     3|  Charlie Lee|     Sales|   TX|110000| 35| 8000|1677624872|\n|  20240927075321357|20240927075321357...|                 2|                      |25309ccf-dec5-4ae...|     2|    Bob Smith|        HR|   NY| 90000| 40| 7000|1677624871|\n|  20240927075321357|20240927075321357...|                 4|                      |25309ccf-dec5-4ae...|     4|  David Brown| Marketing|   FL| 95000| 29| 4000|1677624873|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "impleDataUpd = [\n    (6, \"This is APPEND\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n    (7, \"This is APPEND\", \"Engineering\", \"RJ\", 79000, 53, 15000, 1627694678),\n]\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nusr_up_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\nusr_up_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "###while we did append, one more parquet got created in s3, since it is only an append to the dataset, no delta log file is generated till this step\n\n#25309ccf-dec5-4ae1-8f39-f6391e564bcb-0_0-122-497_20240927080316210.parquet\n\ns3_parquet_path = \"s3://test-ramneek-2/hudi_table/25309ccf-dec5-4ae1-8f39-f6391e564bcb-0_0-122-497_20240927080316210.parquet\"\n\n# Read the Parquet file\ndf2 = spark.read.parquet(s3_parquet_path)\n\ndf2.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id| employee_name| department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|  20240927075321357|20240927075321357...|                 1|                      |25309ccf-dec5-4ae...|     1| Alice Johnson|         IT|   CA|120000| 30| 5000|1677624870|\n|  20240927075321357|20240927075321357...|                 5|                      |25309ccf-dec5-4ae...|     5|     Eve Davis|         IT|   IL|105000| 32| 6000|1677624874|\n|  20240927075321357|20240927075321357...|                 3|                      |25309ccf-dec5-4ae...|     3|   Charlie Lee|      Sales|   TX|110000| 35| 8000|1677624872|\n|  20240927075321357|20240927075321357...|                 2|                      |25309ccf-dec5-4ae...|     2|     Bob Smith|         HR|   NY| 90000| 40| 7000|1677624871|\n|  20240927075321357|20240927075321357...|                 4|                      |25309ccf-dec5-4ae...|     4|   David Brown|  Marketing|   FL| 95000| 29| 4000|1677624873|\n|  20240927080316210|20240927080316210...|                 6|                      |25309ccf-dec5-4ae...|     6|This is APPEND|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20240927080316210|20240927080316210...|                 7|                      |25309ccf-dec5-4ae...|     7|This is APPEND|Engineering|   RJ| 79000| 53|15000|1627694678|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\n###update\n\nimpleDataUpd = [\n    (3, \"this is update on data lake\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n]\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nusr_up_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\nusr_up_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "##after an update operation, no parquet file is generated, only one delta log file is generated. let;s view the real time table.",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from hudi_table_rt;\").show()   ###we can see, there is update at emp_id=3 record",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|       employee_name| department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+-----------+-----+------+---+-----+----------+\n|  20240927075321357|20240927075321357...|                 1|                      |25309ccf-dec5-4ae...|     1|       Alice Johnson|         IT|   CA|120000| 30| 5000|1677624870|\n|  20240927075321357|20240927075321357...|                 5|                      |25309ccf-dec5-4ae...|     5|           Eve Davis|         IT|   IL|105000| 32| 6000|1677624874|\n|  20240927080655269|20240927080655269...|                 3|                      |25309ccf-dec5-4ae...|     3|this is update on...|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20240927075321357|20240927075321357...|                 2|                      |25309ccf-dec5-4ae...|     2|           Bob Smith|         HR|   NY| 90000| 40| 7000|1677624871|\n|  20240927075321357|20240927075321357...|                 4|                      |25309ccf-dec5-4ae...|     4|         David Brown|  Marketing|   FL| 95000| 29| 4000|1677624873|\n|  20240927080316210|20240927080316210...|                 6|                      |25309ccf-dec5-4ae...|     6|      This is APPEND|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20240927080316210|20240927080316210...|                 7|                      |25309ccf-dec5-4ae...|     7|      This is APPEND|Engineering|   RJ| 79000| 53|15000|1627694678|\n|  20240927080655269|20240927080655269...|                 3|                      |25309ccf-dec5-4ae...|     3|this is update on...|      Sales|   RJ| 81000| 30|23000| 827307999|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#lets view the delta log file now.\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n\ns3_path = \"s3://test-ramneek-2/hudi_table/.25309ccf-dec5-4ae1-8f39-f6391e564bcb-0_20240927080316210.log.1_0-156-642\"\n\n# Read the log file\ndf = spark.read.format(\"avro\").load(s3_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o325.load.\n: java.io.FileNotFoundException: No Avro files found. If files don't have .avro extension, set ignoreExtension to true\n\tat org.apache.spark.sql.avro.AvroUtils$.inferAvroSchemaFromFiles(AvroUtils.scala:165)\n\tat org.apache.spark.sql.avro.AvroUtils$.$anonfun$inferSchema$2(AvroUtils.scala:60)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.avro.AvroUtils$.inferSchema(AvroUtils.scala:59)\n\tat org.apache.spark.sql.avro.AvroFileFormat.inferSchema(AvroFileFormat.scala:57)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}