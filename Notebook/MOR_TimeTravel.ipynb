{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "\n%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\n%%configure\n{\n    \"--conf\": \"spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\",\n    \"--enable-glue-datacatalog\" :\"true\",\n    \"--datalake-formats\":\"hudi\"\n}",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nThe following configurations have been updated: {'--conf': 'spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false', '--enable-glue-datacatalog': 'true', '--datalake-formats': 'hudi'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 8156c067-f893-4988-b221-573daa2b48ac\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\n--datalake-formats hudi\nWaiting for session 8156c067-f893-4988-b221-573daa2b48ac to get into ready status...\nSession 8156c067-f893-4988-b221-573daa2b48ac has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"show databases;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+\n|namespace|\n+---------+\n|  default|\n|  hudidb1|\n|  hudidb2|\n|  hudidb3|\n|  hudidb7|\n+---------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "try:\n    import os\n    import sys\n\n\n    import pyspark\n    from pyspark import SparkConf, SparkContext\n    from pyspark.sql import SparkSession\n    from pyspark.sql.functions import col, asc, desc\n    from awsglue.utils import getResolvedOptions\n    from awsglue.dynamicframe import DynamicFrame\n    from awsglue.context import GlueContext\n\n    #from faker import Faker\n\n    print(\"All modules are loaded .....\")\n\nexcept Exception as e:\n    print(\"Some modules are missing {} \".format(e))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "All modules are loaded .....\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "database_name1 = \"hudidb8\"\ntable_name = \"hudi_table\"\nbase_s3_path = \"s3a://test-ramneek-3\"\nfinal_base_path = \"{base_s3_path}/{table_name}\".format(\n    base_s3_path=base_s3_path, table_name=table_name\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "class DataGenerator(object):\n\n    @staticmethod\n    def get_data():\n        # Manually created data\n        return [\n            (1, \"Alice Johnson\", \"IT\", \"CA\", 120000, 30, 5000, 1677624870),\n            (2, \"Bob Smith\", \"HR\", \"NY\", 90000, 40, 7000, 1677624871),\n            (3, \"Charlie Lee\", \"Sales\", \"TX\", 110000, 35, 8000, 1677624872),\n            (4, \"David Brown\", \"Marketing\", \"FL\", 95000, 29, 4000, 1677624873),\n            (5, \"Eve Davis\", \"IT\", \"IL\", 105000, 32, 6000, 1677624874)\n        ]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def create_spark_session():\n    spark = SparkSession \\\n        .builder \\\n        .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n        .getOrCreate()\n    return spark\n\n\nspark = create_spark_session()\nsc = spark.sparkContext\nglueContext = GlueContext(sc)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "hudi_options = {\n    'hoodie.table.name': table_name,\n    \"hoodie.datasource.write.storage.type\": \"MERGE_ON_READ\",\n    'hoodie.datasource.write.recordkey.field': 'emp_id',\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': database_name1,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data = DataGenerator.get_data()\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\ndf = spark.createDataFrame(data=data, schema=columns)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+-------------+----------+-----+------+---+-----+----------+\n|emp_id|employee_name|department|state|salary|age|bonus|        ts|\n+------+-------------+----------+-----+------+---+-----+----------+\n|     1|Alice Johnson|        IT|   CA|120000| 30| 5000|1677624870|\n|     2|    Bob Smith|        HR|   NY| 90000| 40| 7000|1677624871|\n|     3|  Charlie Lee|     Sales|   TX|110000| 35| 8000|1677624872|\n|     4|  David Brown| Marketing|   FL| 95000| 29| 4000|1677624873|\n|     5|    Eve Davis|        IT|   IL|105000| 32| 6000|1677624874|\n+------+-------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"show databases;\").show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+\n|namespace|\n+---------+\n|  default|\n|  hudidb1|\n|  hudidb2|\n|  hudidb3|\n|  hudidb7|\n|  hudidb8|\n+---------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"use hudidb8;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"show tables;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+-------------+-----------+\n|namespace|    tableName|isTemporary|\n+---------+-------------+-----------+\n|  hudidb8|hudi_table_ro|      false|\n|  hudidb8|hudi_table_rt|      false|\n+---------+-------------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from hudi_table_rt;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|employee_name|department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n|  20240930073916918|20240930073916918...|                 1|                      |9f432bb5-43bd-474...|     1|Alice Johnson|        IT|   CA|120000| 30| 5000|1677624870|\n|  20240930073916918|20240930073916918...|                 5|                      |9f432bb5-43bd-474...|     5|    Eve Davis|        IT|   IL|105000| 32| 6000|1677624874|\n|  20240930073916918|20240930073916918...|                 3|                      |9f432bb5-43bd-474...|     3|  Charlie Lee|     Sales|   TX|110000| 35| 8000|1677624872|\n|  20240930073916918|20240930073916918...|                 4|                      |9f432bb5-43bd-474...|     4|  David Brown| Marketing|   FL| 95000| 29| 4000|1677624873|\n|  20240930073916918|20240930073916918...|                 2|                      |9f432bb5-43bd-474...|     2|    Bob Smith|        HR|   NY| 90000| 40| 7000|1677624871|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3_parquet_path = \"s3://test-ramneek-3/hudi_table/9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-30-157_20240930073916918.parquet\"\n\n# Read the Parquet file\ndf1 = spark.read.parquet(s3_parquet_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df1.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|employee_name|department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n|  20240930073916918|20240930073916918...|                 1|                      |9f432bb5-43bd-474...|     1|Alice Johnson|        IT|   CA|120000| 30| 5000|1677624870|\n|  20240930073916918|20240930073916918...|                 5|                      |9f432bb5-43bd-474...|     5|    Eve Davis|        IT|   IL|105000| 32| 6000|1677624874|\n|  20240930073916918|20240930073916918...|                 3|                      |9f432bb5-43bd-474...|     3|  Charlie Lee|     Sales|   TX|110000| 35| 8000|1677624872|\n|  20240930073916918|20240930073916918...|                 4|                      |9f432bb5-43bd-474...|     4|  David Brown| Marketing|   FL| 95000| 29| 4000|1677624873|\n|  20240930073916918|20240930073916918...|                 2|                      |9f432bb5-43bd-474...|     2|    Bob Smith|        HR|   NY| 90000| 40| 7000|1677624871|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+-------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "impleDataUpd = [\n    (6, \"This is APPEND\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n    (7, \"This is APPEND\", \"Engineering\", \"RJ\", 79000, 53, 15000, 1627694678),\n]\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nusr_up_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\nusr_up_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "###while we did append, one more parquet got created in s3, since it is only an append to the dataset, no delta log file is generated till this step\n\n\ns3_parquet_path = \"s3://test-ramneek-3/hudi_table/9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-68-306_20240930074202318.parquet\"\n\n# Read the Parquet file\ndf2 = spark.read.parquet(s3_parquet_path)\n\ndf2.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id| employee_name| department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|  20240930073916918|20240930073916918...|                 1|                      |9f432bb5-43bd-474...|     1| Alice Johnson|         IT|   CA|120000| 30| 5000|1677624870|\n|  20240930073916918|20240930073916918...|                 5|                      |9f432bb5-43bd-474...|     5|     Eve Davis|         IT|   IL|105000| 32| 6000|1677624874|\n|  20240930073916918|20240930073916918...|                 3|                      |9f432bb5-43bd-474...|     3|   Charlie Lee|      Sales|   TX|110000| 35| 8000|1677624872|\n|  20240930073916918|20240930073916918...|                 4|                      |9f432bb5-43bd-474...|     4|   David Brown|  Marketing|   FL| 95000| 29| 4000|1677624873|\n|  20240930073916918|20240930073916918...|                 2|                      |9f432bb5-43bd-474...|     2|     Bob Smith|         HR|   NY| 90000| 40| 7000|1677624871|\n|  20240930074202318|20240930074202318...|                 6|                      |9f432bb5-43bd-474...|     6|This is APPEND|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20240930074202318|20240930074202318...|                 7|                      |9f432bb5-43bd-474...|     7|This is APPEND|Engineering|   RJ| 79000| 53|15000|1627694678|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\n###update\n\nimpleDataUpd = [\n    (3, \"this is update on data lake\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n]\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nusr_up_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\nusr_up_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "##after an update operation, no parquet file is generated, only one delta log file is generated. let;s view the real time table.",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from hudi_table_rt;\").show()   ###we can see, there is update at emp_id=3 record",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id|       employee_name| department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+-----------+-----+------+---+-----+----------+\n|  20240930073916918|20240930073916918...|                 1|                      |9f432bb5-43bd-474...|     1|       Alice Johnson|         IT|   CA|120000| 30| 5000|1677624870|\n|  20240930073916918|20240930073916918...|                 5|                      |9f432bb5-43bd-474...|     5|           Eve Davis|         IT|   IL|105000| 32| 6000|1677624874|\n|  20240930074340349|20240930074340349...|                 3|                      |9f432bb5-43bd-474...|     3|this is update on...|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20240930073916918|20240930073916918...|                 4|                      |9f432bb5-43bd-474...|     4|         David Brown|  Marketing|   FL| 95000| 29| 4000|1677624873|\n|  20240930073916918|20240930073916918...|                 2|                      |9f432bb5-43bd-474...|     2|           Bob Smith|         HR|   NY| 90000| 40| 7000|1677624871|\n|  20240930074202318|20240930074202318...|                 6|                      |9f432bb5-43bd-474...|     6|      This is APPEND|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20240930074202318|20240930074202318...|                 7|                      |9f432bb5-43bd-474...|     7|      This is APPEND|Engineering|   RJ| 79000| 53|15000|1627694678|\n|  20240930074340349|20240930074340349...|                 3|                      |9f432bb5-43bd-474...|     3|this is update on...|      Sales|   RJ| 81000| 30|23000| 827307999|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## hudi_table_ro will remain in its original form, to sync it we will need to do the compaction.",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from hudi_table_ro;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|emp_id| employee_name| department|state|salary|age|bonus|        ts|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|  20240930073916918|20240930073916918...|                 1|                      |9f432bb5-43bd-474...|     1| Alice Johnson|         IT|   CA|120000| 30| 5000|1677624870|\n|  20240930073916918|20240930073916918...|                 5|                      |9f432bb5-43bd-474...|     5|     Eve Davis|         IT|   IL|105000| 32| 6000|1677624874|\n|  20240930073916918|20240930073916918...|                 3|                      |9f432bb5-43bd-474...|     3|   Charlie Lee|      Sales|   TX|110000| 35| 8000|1677624872|\n|  20240930073916918|20240930073916918...|                 4|                      |9f432bb5-43bd-474...|     4|   David Brown|  Marketing|   FL| 95000| 29| 4000|1677624873|\n|  20240930073916918|20240930073916918...|                 2|                      |9f432bb5-43bd-474...|     2|     Bob Smith|         HR|   NY| 90000| 40| 7000|1677624871|\n|  20240930074202318|20240930074202318...|                 6|                      |9f432bb5-43bd-474...|     6|This is APPEND|      Sales|   RJ| 81000| 30|23000| 827307999|\n|  20240930074202318|20240930074202318...|                 7|                      |9f432bb5-43bd-474...|     7|This is APPEND|Engineering|   RJ| 79000| 53|15000|1627694678|\n+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\"\"\"\nIn Apache Hudi, the real-time (RT) table reflects the latest data, including all updates and deletes that have been performed. The read-optimized (RO) table, on the other hand, contains only the base data files, excluding any changes from the delta log files until compaction is triggered.\n\nThe compaction process merges the base data files with the log files (delta logs) to produce new base files that reflect the most up-to-date state of the data. The RO table will not reflect updates until this compaction is performed.\n\nTo synchronize your RO table with the RT table, you need to trigger a compaction. You can do this by setting up compaction in your Hudi options or triggering it manually.\n\nSteps to Trigger Compaction Manually:\nUpdate your Hudi write options to include compaction configuration, if it is not already set.\nRun a compaction job to merge the log files and base files.\n\n\"\"\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "'\\nIn Apache Hudi, the real-time (RT) table reflects the latest data, including all updates and deletes that have been performed. The read-optimized (RO) table, on the other hand, contains only the base data files, excluding any changes from the delta log files until compaction is triggered.\\n\\nThe compaction process merges the base data files with the log files (delta logs) to produce new base files that reflect the most up-to-date state of the data. The RO table will not reflect updates until this compaction is performed.\\n\\nTo synchronize your RO table with the RT table, you need to trigger a compaction. You can do this by setting up compaction in your Hudi options or triggering it manually.\\n\\nSteps to Trigger Compaction Manually:\\nUpdate your Hudi write options to include compaction configuration, if it is not already set.\\nRun a compaction job to merge the log files and base files.\\n\\n'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Load the Hudi table and list commits to find the commit time\nmeta_df = spark.read.format(\"hudi\").load(final_base_path)\nmeta_df.createOrReplaceTempView(\"hudi_metadata\")\n\n# List all the commit times (the instant times)\ncommit_time_df = spark.sql(\"SELECT distinct(_hoodie_commit_time) as commit_time FROM hudi_metadata order by commit_time desc\")\ncommit_time_df.show(truncate=False)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|commit_time      |\n+-----------------+\n|20240930075453211|\n|20240930074202318|\n|20240930073916918|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "commit_time = \"20240930073916918\"  # Example commit time\n\n# Perform a time travel query using the commit time\ntime_travel_df = spark.read.format(\"hudi\") \\\n    .option(\"as.of.instant\", commit_time) \\\n    .load(final_base_path)\n\n# Display the results as of the commit time\ntime_travel_df.show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+---------------------+------------------+----------------------+-------------------------------------------------------------------------+------+-------------+----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                        |emp_id|employee_name|department|state|salary|age|bonus|ts        |\n+-------------------+---------------------+------------------+----------------------+-------------------------------------------------------------------------+------+-------------+----------+-----+------+---+-----+----------+\n|20240930073916918  |20240930073916918_0_0|1                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-30-157_20240930073916918.parquet|1     |Alice Johnson|IT        |CA   |120000|30 |5000 |1677624870|\n|20240930073916918  |20240930073916918_0_1|5                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-30-157_20240930073916918.parquet|5     |Eve Davis    |IT        |IL   |105000|32 |6000 |1677624874|\n|20240930073916918  |20240930073916918_0_2|3                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-30-157_20240930073916918.parquet|3     |Charlie Lee  |Sales     |TX   |110000|35 |8000 |1677624872|\n|20240930073916918  |20240930073916918_0_3|4                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-30-157_20240930073916918.parquet|4     |David Brown  |Marketing |FL   |95000 |29 |4000 |1677624873|\n|20240930073916918  |20240930073916918_0_4|2                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-30-157_20240930073916918.parquet|2     |Bob Smith    |HR        |NY   |90000 |40 |7000 |1677624871|\n+-------------------+---------------------+------------------+----------------------+-------------------------------------------------------------------------+------+-------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "commit_time = \"20240930074202318\"  # Example commit time\n\n# Perform a time travel query using the commit time\ntime_travel_df = spark.read.format(\"hudi\") \\\n    .option(\"as.of.instant\", commit_time) \\\n    .load(final_base_path)\n\n# Display the results as of the commit time\ntime_travel_df.show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+---------------------+------------------+----------------------+-------------------------------------------------------------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                        |emp_id|employee_name |department |state|salary|age|bonus|ts        |\n+-------------------+---------------------+------------------+----------------------+-------------------------------------------------------------------------+------+--------------+-----------+-----+------+---+-----+----------+\n|20240930073916918  |20240930073916918_0_0|1                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-68-306_20240930074202318.parquet|1     |Alice Johnson |IT         |CA   |120000|30 |5000 |1677624870|\n|20240930073916918  |20240930073916918_0_1|5                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-68-306_20240930074202318.parquet|5     |Eve Davis     |IT         |IL   |105000|32 |6000 |1677624874|\n|20240930073916918  |20240930073916918_0_2|3                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-68-306_20240930074202318.parquet|3     |Charlie Lee   |Sales      |TX   |110000|35 |8000 |1677624872|\n|20240930073916918  |20240930073916918_0_3|4                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-68-306_20240930074202318.parquet|4     |David Brown   |Marketing  |FL   |95000 |29 |4000 |1677624873|\n|20240930073916918  |20240930073916918_0_4|2                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-68-306_20240930074202318.parquet|2     |Bob Smith     |HR         |NY   |90000 |40 |7000 |1677624871|\n|20240930074202318  |20240930074202318_0_5|6                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-68-306_20240930074202318.parquet|6     |This is APPEND|Sales      |RJ   |81000 |30 |23000|827307999 |\n|20240930074202318  |20240930074202318_0_6|7                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-68-306_20240930074202318.parquet|7     |This is APPEND|Engineering|RJ   |79000 |53 |15000|1627694678|\n+-------------------+---------------------+------------------+----------------------+-------------------------------------------------------------------------+------+--------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "commit_time = \"20240930075453211\"  # Example commit time\n\n# Perform a time travel query using the commit time\ntime_travel_df = spark.read.format(\"hudi\") \\\n    .option(\"as.of.instant\", commit_time) \\\n    .load(final_base_path)\n\n# Display the results as of the commit time\ntime_travel_df.show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+---------------------+------------------+----------------------+--------------------------------------------------------------------------+------+---------------------------+-----------+-----+------+---+-----+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                         |emp_id|employee_name              |department |state|salary|age|bonus|ts        |\n+-------------------+---------------------+------------------+----------------------+--------------------------------------------------------------------------+------+---------------------------+-----------+-----+------+---+-----+----------+\n|20240930073916918  |20240930073916918_0_0|1                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-163-635_20240930075214992.parquet|1     |Alice Johnson              |IT         |CA   |120000|30 |5000 |1677624870|\n|20240930073916918  |20240930073916918_0_1|5                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-163-635_20240930075214992.parquet|5     |Eve Davis                  |IT         |IL   |105000|32 |6000 |1677624874|\n|20240930075453211  |20240930075453211_0_5|3                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0                                    |3     |this is update on data lake|Sales      |RJ   |81000 |30 |23000|827307999 |\n|20240930073916918  |20240930073916918_0_3|4                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-163-635_20240930075214992.parquet|4     |David Brown                |Marketing  |FL   |95000 |29 |4000 |1677624873|\n|20240930073916918  |20240930073916918_0_4|2                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-163-635_20240930075214992.parquet|2     |Bob Smith                  |HR         |NY   |90000 |40 |7000 |1677624871|\n|20240930074202318  |20240930074202318_0_5|6                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-163-635_20240930075214992.parquet|6     |This is APPEND             |Sales      |RJ   |81000 |30 |23000|827307999 |\n|20240930074202318  |20240930074202318_0_6|7                 |                      |9f432bb5-43bd-4748-88d8-829d4f0d73ba-0_0-163-635_20240930075214992.parquet|7     |This is APPEND             |Engineering|RJ   |79000 |53 |15000|1627694678|\n+-------------------+---------------------+------------------+----------------------+--------------------------------------------------------------------------+------+---------------------------+-----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\"\"\"\nspark.read.format(\"hudi\").load(final_base_path) allows you to access both the data and the associated metadata in a unified way, leveraging Hudi's design to provide features like time travel and versioning efficiently. \n\nmeta_df = spark.read.format(\"hudi\").load(final_base_path):\n\nThis line reads the Hudi table from the S3 path (final_base_path) into a Spark DataFrame.\nHudi maintains metadata along with the data itself, including commit times (known as instant times) stored in the _hoodie_commit_time field.\nThe Hudi table can store multiple versions of the data through these commit times.\nmeta_df.createOrReplaceTempView(\"hudi_metadata\"):\n\nHere, a temporary SQL view called \"hudi_metadata\" is created from the DataFrame meta_df. This allows us to run SQL queries directly on the metadata of the Hudi table.\nSQL queries are useful to manipulate and extract data in a tabular manner.\ncommit_time_df = spark.sql(\"SELECT distinct(_hoodie_commit_time) as commit_time FROM hudi_metadata order by commit_time desc\"):\n\nThis SQL query fetches all distinct commit times (_hoodie_commit_time) from the Hudi table's metadata. It sorts them in descending order (order by commit_time desc), so the most recent commit appears first.\nCommit times are important because Hudi stores different versions of the data at each commit. These commit times are critical for time travel queries.\ncommit_time_df.show(truncate=False):\n\nThis prints the distinct commit times (versions of the dataset) without truncating the results. This list shows all available points in time where changes (commits) were made to the dataset.\nYou’ll use one of these commit times to perform a time travel query.\"\"\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "'\\nmeta_df = spark.read.format(\"hudi\").load(final_base_path):\\n\\nThis line reads the Hudi table from the S3 path (final_base_path) into a Spark DataFrame.\\nHudi maintains metadata along with the data itself, including commit times (known as instant times) stored in the _hoodie_commit_time field.\\nThe Hudi table can store multiple versions of the data through these commit times.\\nmeta_df.createOrReplaceTempView(\"hudi_metadata\"):\\n\\nHere, a temporary SQL view called \"hudi_metadata\" is created from the DataFrame meta_df. This allows us to run SQL queries directly on the metadata of the Hudi table.\\nSQL queries are useful to manipulate and extract data in a tabular manner.\\ncommit_time_df = spark.sql(\"SELECT distinct(_hoodie_commit_time) as commit_time FROM hudi_metadata order by commit_time desc\"):\\n\\nThis SQL query fetches all distinct commit times (_hoodie_commit_time) from the Hudi table\\'s metadata. It sorts them in descending order (order by commit_time desc), so the most recent commit appears first.\\nCommit times are important because Hudi stores different versions of the data at each commit. These commit times are critical for time travel queries.\\ncommit_time_df.show(truncate=False):\\n\\nThis prints the distinct commit times (versions of the dataset) without truncating the results. This list shows all available points in time where changes (commits) were made to the dataset.\\nYou’ll use one of these commit times to perform a time travel query.'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\"\"\"\n.option(\"as.of.instant\", commit_time):\n\nThis is the key line for time travel in Hudi. By specifying as.of.instant, you are telling Hudi to load the table’s state as it existed at the exact commit_time provided.\nThe time travel query allows you to view the data as it was at that commit time.\n\n\"\"\"",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "###apache hudi cleaner",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}