{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "\n%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\n%%configure\n{\n    \"--conf\": \"spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\",\n    \"--enable-glue-datacatalog\" :\"true\",\n    \"--datalake-formats\":\"hudi\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nThe following configurations have been updated: {'--conf': 'spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false', '--enable-glue-datacatalog': 'true', '--datalake-formats': 'hudi'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 4ee7d88f-4f04-478d-82e1-d0d1557cf146\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\n--datalake-formats hudi\nWaiting for session 4ee7d88f-4f04-478d-82e1-d0d1557cf146 to get into ready status...\nSession 4ee7d88f-4f04-478d-82e1-d0d1557cf146 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.session import SparkSession",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark = SparkSession.builder.config('spark.serializer','org.apache.spark.serializer.KryoSerializer').config('spark.sql.hive.convertMetastoreParquet','false').config('spark.sql.legacy.pathOptionBehavior.enabled', 'true').getOrCreate()\nsc = spark.sparkContext\nglueContext = GlueContext(sc)\njob = Job(glueContext)\nlogger = glueContext.get_logger()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "##creating a df\n\nfrom pyspark.sql.functions import udf\nimport time\n\nrandom_udf = udf(lambda: str(int(time.time() * 1000000)), StringType()) \n\ndim_customer_schema = StructType([\n        StructField('customer_id', StringType(), False),\n        StructField('first_name', StringType(), True),\n        StructField('last_name', StringType(), True),\n        StructField('city', StringType(), True),\n        StructField('country', StringType(), True),\n        StructField('eff_start_date', DateType(), True),\n        StructField('eff_end_date', DateType(), True),\n        StructField('timestamp', TimestampType(), True),\n        StructField('is_current', BooleanType(), True),\n    ])\n\ncustomer_dim_df = spark.createDataFrame([('1', 'John', 'Smith', \n                    'London', 'UK', \n                    datetime.strptime('2020-09-27', '%Y-%m-%d'),\n                    datetime.strptime('2999-12-31', '%Y-%m-%d'),\n                    datetime.strptime('2020-12-08 09:15:32', '%Y-%m-%d %H:%M:%S'),\n                    True),\n                    ('2', 'Susan', 'Chas', \n                    'Seattle', 'US',\n                    datetime.strptime('2020-10-14', '%Y-%m-%d'),\n                    datetime.strptime('2999-12-31', '%Y-%m-%d'),\n                    datetime.strptime('2020-12-08 09:15:32', '%Y-%m-%d %H:%M:%S'),\n                    True)], dim_customer_schema)\n\ncustomer_hudi_df = customer_dim_df.withColumn(\"customer_dim_key\", random_udf())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import col, to_timestamp, monotonically_increasing_id, to_date, when\nfrom pyspark.sql.functions import *\n\nfrom pyspark.sql.types import *\nfrom datetime import datetime\nimport boto3\nfrom functools import reduce",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "customer_hudi_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|          timestamp|is_current|customer_dim_key|\n+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n|          1|      John|    Smith| London|     UK|    2020-09-27|  2999-12-31|2020-12-08 09:15:32|      true|1727768187773414|\n|          2|     Susan|     Chas|Seattle|     US|    2020-10-14|  2999-12-31|2020-12-08 09:15:32|      true|1727768187889384|\n+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "database_name1 = \"hudidb9\"\ntable_name = \"customer_table\"\nbase_s3_path = \"s3a://test-ramneek-3\"\nfinal_base_path = \"{base_s3_path}/{table_name}\".format(\n    base_s3_path=base_s3_path, table_name=table_name\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "hudi_options = {\n    'hoodie.table.name': table_name,\n    \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n    'hoodie.datasource.write.recordkey.field': 'customer_dim_key',\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'timestamp',\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': database_name1,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "customer_hudi_df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"use hudidb9;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"show tables;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+--------------+-----------+\n|namespace|     tableName|isTemporary|\n+---------+--------------+-----------+\n|  hudidb9|customer_table|      false|\n+---------+--------------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from customer_table;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+-------+-------+--------------+------------+----------------+----------+----------------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|       timestamp|is_current|customer_dim_key|\n+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+-------+-------+--------------+------------+----------------+----------+----------------+\n|  20241001075114800|20241001075114800...|  1727769086113419|                      |9eb8d2a1-f5df-40a...|          1|      John|    Smith| London|     UK|    2020-09-27|  2999-12-31|1607418932000000|      true|1727769086113419|\n|  20241001075114800|20241001075114800...|  1727769086114582|                      |9eb8d2a1-f5df-40a...|          2|     Susan|     Chas|Seattle|     US|    2020-10-14|  2999-12-31|1607418932000000|      true|1727769086114582|\n+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+-------+-------+--------------+------------+----------------+----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "##fact table:\n\nfact_sales_schema = StructType([\n        StructField('item_id', StringType(), True),\n        StructField('quantity', IntegerType(), True),\n        StructField('price', DoubleType(), True),\n        StructField('timestamp', TimestampType(), True),\n        StructField('customer_id', StringType(), True)\n    ])\n\nsales_fact_df = spark.createDataFrame([('100', 25, 123.46,\n                    datetime.strptime('2020-11-17 09:15:32', '%Y-%m-%d %H:%M:%S'), '1'),\n                                       ('101', 300, 123.46,\n                    datetime.strptime('2020-10-28 09:15:32', '%Y-%m-%d %H:%M:%S'), '1'),\n                                      ('102', 5, 1038.0,\n                    datetime.strptime('2020-12-08 09:15:32', '%Y-%m-%d %H:%M:%S'), '2')], \n                    fact_sales_schema)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "sales_fact_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+--------+------+-------------------+-----------+\n|item_id|quantity| price|          timestamp|customer_id|\n+-------+--------+------+-------------------+-----------+\n|    100|      25|123.46|2020-11-17 09:15:32|          1|\n|    101|     300|123.46|2020-10-28 09:15:32|          1|\n|    102|       5|1038.0|2020-12-08 09:15:32|          2|\n+-------+--------+------+-------------------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "database_name1 = \"hudidb9\"\ntable_name1 = \"sales_table\"\nbase_s3_path = \"s3a://test-ramneek-3\"\nfinal_base_path = \"{base_s3_path}/{table_name}\".format(\n    base_s3_path=base_s3_path, table_name=table_name\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "hudi_options = {\n    'hoodie.table.name': table_name1,\n    \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n    'hoodie.datasource.write.recordkey.field': 'customer_id',\n    'hoodie.datasource.write.table.name': table_name1,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'timestamp',\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': database_name1,\n    'hoodie.datasource.hive_sync.table': table_name1,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "sales_fact_df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from sales_table;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------+------+----------------+-----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|item_id|quantity| price|       timestamp|customer_id|\n+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------+------+----------------+-----------+\n|  20241001080038210|20241001080038210...|                 1|                      |7710c2ba-9b65-432...|    100|      25|123.46|1605604532000000|          1|\n|  20241001080038210|20241001080038210...|                 2|                      |7710c2ba-9b65-432...|    102|       5|1038.0|1607418932000000|          2|\n+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------+------+----------------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import when\n\njoin_cond = [sales_fact_df.customer_id == customer_hudi_df.customer_id,\n             sales_fact_df.timestamp >= customer_hudi_df.eff_start_date,\n             sales_fact_df.timestamp < customer_hudi_df.eff_end_date]\n\ncustomers_dim_key_df = (sales_fact_df\n                          .join(customer_hudi_df, join_cond, 'leftouter')\n                          .select(sales_fact_df['*'],\n                            when(customer_hudi_df.customer_dim_key.isNull(), '-1')\n                                  .otherwise(customer_hudi_df.customer_dim_key)\n                                  .alias(\"customer_dim_key\") )\n                       )",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "customers_dim_key_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+--------+------+-------------------+-----------+----------------+\n|item_id|quantity| price|          timestamp|customer_id|customer_dim_key|\n+-------+--------+------+-------------------+-----------+----------------+\n|    100|      25|123.46|2020-11-17 09:15:32|          1|1727769805558024|\n|    101|     300|123.46|2020-10-28 09:15:32|          1|1727769805558024|\n|    102|       5|1038.0|2020-12-08 09:15:32|          2|1727769805622081|\n+-------+--------+------+-------------------+-----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "new_customer_dim_df = spark.createDataFrame([('3', 'Bastian', 'Back', 'Berlin', 'GE',\n                    datetime.strptime(datetime.today().strftime('%Y-%m-%d'), '%Y-%m-%d'),\n                    datetime.strptime('2999-12-31', '%Y-%m-%d'),\n                    datetime.strptime('2020-12-09 09:15:32', '%Y-%m-%d %H:%M:%S'), True),\n                    ('2', 'Susan', 'Chas','Paris', 'FR',\n                    datetime.strptime(datetime.today().strftime('%Y-%m-%d'), '%Y-%m-%d'),\n                    datetime.strptime('2999-12-31', '%Y-%m-%d'),\n                    datetime.strptime('2020-12-09 10:15:32', '%Y-%m-%d %H:%M:%S'), True)],\n                dim_customer_schema)\nnew_customer_dim_df = new_customer_dim_df.withColumn(\"customer_dim_key\", random_udf())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "new_customer_dim_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----------+---------+------+-------+--------------+------------+-------------------+----------+----------------+\n|customer_id|first_name|last_name|  city|country|eff_start_date|eff_end_date|          timestamp|is_current|customer_dim_key|\n+-----------+----------+---------+------+-------+--------------+------------+-------------------+----------+----------------+\n|          3|   Bastian|     Back|Berlin|     GE|    2024-10-01|  2999-12-31|2020-12-09 09:15:32|      true|1727769970838559|\n|          2|     Susan|     Chas| Paris|     FR|    2024-10-01|  2999-12-31|2020-12-09 10:15:32|      true|1727769970882771|\n+-----------+----------+---------+------+-------+--------------+------------+-------------------+----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\"\"\"What is Slowly Changing Dimension ?\nA Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a data warehouse. It is considered and implemented as one of the most critical ETL tasks in tracking the history of dimension records.\nIn this Case Customer Susan moved to Paris from Seattle we can handel this folloowing ways\n\nType 1 SCDs - Overwriting\nIn a Type 1 SCD the new data overwrites the existing data. Thus the existing data is lost as it is not stored anywhere else. This is the default type of dimension you create. You do not need to specify any additional information to create a Type 1 SCD.\n\nType 2 SCDs - Creating another dimension record\nA Type 2 SCD retains the full history of values. When the value of a chosen attribute changes, the current record is closed. A new record is created with the changed data values and this new record becomes the current record. Each record contains the effective time and expiration time to identify the time period between which the record was active.\n\nType 3 SCDs - Creating a current value field\nA Type 3 SCD stores two versions of values for certain selected level attributes. Each record stores the previous value and the current value of the selected attribute. When the value of any of the selected attributes changes, the current value is stored as the old value and the new value becomes the current value.\n\n\nWhat is Slowly Changing Dimension ?\nA Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a data warehouse. It is considered and implemented as one of the most critical ETL tasks in tracking the history of dimension records.\nIn this Case Customer Susan moved to Paris from Seattle we can handel this folloowing ways\n\nType 1 SCDs - Overwriting\nIn a Type 1 SCD the new data overwrites the existing data. Thus the existing data is lost as it is not stored anywhere else. This is the default type of dimension you create. You do not need to specify any additional information to create a Type 1 SCD.\n\nType 2 SCDs - Creating another dimension record\nA Type 2 SCD retains the full history of values. When the value of a chosen attribute changes, the current record is closed. A new record is created with the changed data values and this new record becomes the current record. Each record contains the effective time and expiration time to identify the time period between which the record was active.\n\nType 3 SCDs - Creating a current value field\nA Type 3 SCD stores two versions of values for certain selected level attributes. Each record stores the previous value and the current value of the selected attribute. When the value of any of the selected attributes changes, the current value is stored as the old value and the new value becomes the current value.\n\n\"\"\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "'What is Slowly Changing Dimension ?\\nA Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a data warehouse. It is considered and implemented as one of the most critical ETL tasks in tracking the history of dimension records.\\nIn this Case Customer Susan moved to Paris from Seattle we can handel this folloowing ways\\n\\nType 1 SCDs - Overwriting\\nIn a Type 1 SCD the new data overwrites the existing data. Thus the existing data is lost as it is not stored anywhere else. This is the default type of dimension you create. You do not need to specify any additional information to create a Type 1 SCD.\\n\\nType 2 SCDs - Creating another dimension record\\nA Type 2 SCD retains the full history of values. When the value of a chosen attribute changes, the current record is closed. A new record is created with the changed data values and this new record becomes the current record. Each record contains the effective time and expiration time to identify the time period between which the record was active.\\n\\nType 3 SCDs - Creating a current value field\\nA Type 3 SCD stores two versions of values for certain selected level attributes. Each record stores the previous value and the current value of the selected attribute. When the value of any of the selected attributes changes, the current value is stored as the old value and the new value becomes the current value.\\n\\n\\nWhat is Slowly Changing Dimension ?\\nA Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a data warehouse. It is considered and implemented as one of the most critical ETL tasks in tracking the history of dimension records.\\nIn this Case Customer Susan moved to Paris from Seattle we can handel this folloowing ways\\n\\nType 1 SCDs - Overwriting\\nIn a Type 1 SCD the new data overwrites the existing data. Thus the existing data is lost as it is not stored anywhere else. This is the default type of dimension you create. You do not need to specify any additional information to create a Type 1 SCD.\\n\\nType 2 SCDs - Creating another dimension record\\nA Type 2 SCD retains the full history of values. When the value of a chosen attribute changes, the current record is closed. A new record is created with the changed data values and this new record becomes the current record. Each record contains the effective time and expiration time to identify the time period between which the record was active.\\n\\nType 3 SCDs - Creating a current value field\\nA Type 3 SCD stores two versions of values for certain selected level attributes. Each record stores the previous value and the current value of the selected attribute. When the value of any of the selected attributes changes, the current value is stored as the old value and the new value becomes the current value.\\n\\n'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "join_cond = [customer_hudi_df.customer_id == new_customer_dim_df.customer_id,\n             customer_hudi_df.is_current == True]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## Find customer records to update\ncustomers_to_update_df = (customer_hudi_df\n                          .join(new_customer_dim_df, join_cond)\n                          .select(customer_hudi_df.customer_id,\n                                  customer_hudi_df.first_name,\n                                  customer_hudi_df.last_name,\n                                  customer_hudi_df.city,\n                                  customer_hudi_df.country,\n                                  customer_hudi_df.eff_start_date,\n                                  new_customer_dim_df.eff_start_date.alias('eff_end_date'),\n                                  customer_hudi_df.customer_dim_key,\n                                  customer_hudi_df.timestamp)\n                          .withColumn('is_current', lit(False))\n                         )",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "customers_to_update_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----------+---------+-------+-------+--------------+------------+----------------+-------------------+----------+\n|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|customer_dim_key|          timestamp|is_current|\n+-----------+----------+---------+-------+-------+--------------+------------+----------------+-------------------+----------+\n|          2|     Susan|     Chas|Seattle|     US|    2020-10-14|  2024-10-01|1727770091620484|2020-12-08 09:15:32|     false|\n+-----------+----------+---------+-------+-------+--------------+------------+----------------+-------------------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## Union with new customer records\nmerged_customers_df = new_customer_dim_df. unionByName(customers_to_update_df)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\nmerged_customers_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|          timestamp|is_current|customer_dim_key|\n+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n|          3|   Bastian|     Back| Berlin|     GE|    2024-10-01|  2999-12-31|2020-12-09 09:15:32|      true|1727770126740537|\n|          2|     Susan|     Chas|  Paris|     FR|    2024-10-01|  2999-12-31|2020-12-09 10:15:32|      true|1727770126804428|\n|          2|     Susan|     Chas|Seattle|     US|    2020-10-14|  2024-10-01|2020-12-08 09:15:32|     false|1727770125588014|\n+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "database_name1 = \"hudidb9\"\ntable_name = \"customer_table\"\nbase_s3_path = \"s3a://test-ramneek-3\"\nfinal_base_path = \"{base_s3_path}/{table_name}\".format(\n    base_s3_path=base_s3_path, table_name=table_name\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "hudi_options = {\n    'hoodie.table.name': table_name,\n    \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n    'hoodie.datasource.write.recordkey.field': 'customer_dim_key',\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'timestamp',\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': database_name1,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "merged_customers_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select * from customer_table;\").show() #you can see, Susan has 2 records, one with city Seattle, other with Paris and the former record is set as false.",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 53,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+-------+-------+--------------+------------+----------------+----------+----------------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|       timestamp|is_current|customer_dim_key|\n+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+-------+-------+--------------+------------+----------------+----------+----------------+\n|  20241001075114800|20241001075114800...|  1727769086113419|                      |9eb8d2a1-f5df-40a...|          1|      John|    Smith| London|     UK|    2020-09-27|  2999-12-31|1607418932000000|      true|1727769086113419|\n|  20241001075114800|20241001075114800...|  1727769086114582|                      |9eb8d2a1-f5df-40a...|          2|     Susan|     Chas|Seattle|     US|    2020-10-14|  2999-12-31|1607418932000000|      true|1727769086114582|\n|  20241001081602346|20241001081602346...|  1727770564991404|                      |9eb8d2a1-f5df-40a...|          3|   Bastian|     Back| Berlin|     GE|    2024-10-01|  2999-12-31|1607505332000000|      true|1727770564991404|\n|  20241001081602346|20241001081602346...|  1727770563077256|                      |9eb8d2a1-f5df-40a...|          2|     Susan|     Chas|Seattle|     US|    2020-10-14|  2024-10-01|1607418932000000|     false|1727770563077256|\n|  20241001081602346|20241001081602346...|  1727770565038846|                      |9eb8d2a1-f5df-40a...|          2|     Susan|     Chas|  Paris|     FR|    2024-10-01|  2999-12-31|1607508932000000|      true|1727770565038846|\n+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+-------+-------+--------------+------------+----------------+----------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}