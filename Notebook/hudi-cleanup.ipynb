{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "\n%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\n%%configure\n{\n    \"--conf\": \"spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false\",\n    \"--enable-glue-datacatalog\" :\"true\",\n    \"--datalake-formats\":\"hudi\"\n}",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nThe following configurations have been updated: {'--conf': 'spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false', '--enable-glue-datacatalog': 'true', '--datalake-formats': 'hudi'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"show databases;\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+\n|namespace|\n+---------+\n|  default|\n|  hudidb1|\n|  hudidb2|\n|  hudidb3|\n|  hudidb7|\n|  hudidb8|\n+---------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "try:\n    import os\n    import sys\n\n\n    import pyspark\n    from pyspark import SparkConf, SparkContext\n    from pyspark.sql import SparkSession\n    from pyspark.sql.functions import col, asc, desc\n    from awsglue.utils import getResolvedOptions\n    from awsglue.dynamicframe import DynamicFrame\n    from awsglue.context import GlueContext\n\n    #from faker import Faker\n\n    print(\"All modules are loaded .....\")\n\nexcept Exception as e:\n    print(\"Some modules are missing {} \".format(e))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "All modules are loaded .....\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "database_name1 = \"hudidb8\"\ntable_name = \"hudi_table\"\nbase_s3_path = \"s3a://test-ramneek-3\"\nfinal_base_path = \"{base_s3_path}/{table_name}\".format(\n    base_s3_path=base_s3_path, table_name=table_name\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "class DataGenerator(object):\n\n    @staticmethod\n    def get_data():\n        # Manually created data\n        return [\n            (1, \"Alice Johnson\", \"IT\", \"CA\", 120000, 30, 5000, 1677624870),\n            (2, \"Bob Smith\", \"HR\", \"NY\", 90000, 40, 7000, 1677624871),\n            (3, \"Charlie Lee\", \"Sales\", \"TX\", 110000, 35, 8000, 1677624872),\n            (4, \"David Brown\", \"Marketing\", \"FL\", 95000, 29, 4000, 1677624873),\n            (5, \"Eve Davis\", \"IT\", \"IL\", 105000, 32, 6000, 1677624874)\n        ]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def create_spark_session():\n    spark = SparkSession \\\n        .builder \\\n        .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n        .getOrCreate()\n    return spark\n\n\nspark = create_spark_session()\nsc = spark.sparkContext\nglueContext = GlueContext(sc)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "hudi_options = {\n    'hoodie.table.name': table_name,\n    \"hoodie.datasource.write.storage.type\": \"MERGE_ON_READ\",\n    'hoodie.datasource.write.recordkey.field': 'emp_id',\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': database_name1,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data = DataGenerator.get_data()\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\ndf = spark.createDataFrame(data=data, schema=columns)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+-------------+----------+-----+------+---+-----+----------+\n|emp_id|employee_name|department|state|salary|age|bonus|        ts|\n+------+-------------+----------+-----+------+---+-----+----------+\n|     1|Alice Johnson|        IT|   CA|120000| 30| 5000|1677624870|\n|     2|    Bob Smith|        HR|   NY| 90000| 40| 7000|1677624871|\n|     3|  Charlie Lee|     Sales|   TX|110000| 35| 8000|1677624872|\n|     4|  David Brown| Marketing|   FL| 95000| 29| 4000|1677624873|\n|     5|    Eve Davis|        IT|   IL|105000| 32| 6000|1677624874|\n+------+-------------+----------+-----+------+---+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "impleDataUpd = [\n    (6, \"This is APPEND\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n    (7, \"This is APPEND\", \"Engineering\", \"RJ\", 79000, 53, 15000, 1627694678),\n]\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nusr_up_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\nusr_up_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "impleDataUpd = [\n    (8, \"This is APPEND1\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n    (9, \"This is APPEND1\", \"Engineering\", \"RJ\", 79000, 53, 15000, 1627694678),\n]\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nusr_up_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\nusr_up_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "impleDataUpd = [\n    (10, \"This is APPEND1\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n    (11, \"This is APPEND1\", \"Engineering\", \"RJ\", 79000, 53, 15000, 1627694678),\n]\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nusr_up_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\nusr_up_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "impleDataUpd = [\n    (12, \"This is APPEND1\", \"Sales\", \"RJ\", 81000, 30, 23000, 827307999),\n    (13, \"This is APPEND1\", \"Engineering\", \"RJ\", 79000, 53, 15000, 1627694678),\n]\n\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\"]\nusr_up_df = spark.createDataFrame(data=impleDataUpd, schema=columns)\nusr_up_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(final_base_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "hudi_options_cleaner = {\n    'hoodie.table.name': table_name,\n    \"hoodie.datasource.write.storage.type\": \"MERGE_ON_READ\",\n    'hoodie.datasource.write.recordkey.field': 'emp_id',\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'ts',\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': database_name1,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n\n    # Cleaner configurations\n    'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS',  # Use KEEP_LATEST_COMMITS policy\n    'hoodie.cleaner.max.commits': '3',  # Keep the latest 3 commits\n    'hoodie.cleaner.parallelism': '4',  # Number of parallel cleaner threads\n    \n}\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "usr_up_df.write.format(\"hudi\").options(**hudi_options_cleaner).mode(\"append\").save(final_base_path)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}